{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module 2 - Project - Text Pre-processor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshi5oct/Representation-Learning-for-NLP/blob/master/Module_2_Project_Text_Pre_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNDJ-WfhnK5E",
        "colab_type": "code",
        "outputId": "09868c59-46b3-4392-a8aa-44cbca031e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install contractions\n",
        "!pip install spacy\n",
        "!pip install textsearch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2a/ba0a3812e2a1de2cc4ee0ded0bdb750a7cef1631c13c78a4fc4ab042adec/contractions-0.0.21-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.21\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.1MB/s \n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 43.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch\n",
            "Successfully installed Unidecode-1.1.1 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLkGGqoIp-rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
        "ps = nltk.porter.PorterStemmer()\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "\n",
        "def spacy_lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "\n",
        "def simple_stemming(text, stemmer=ps):\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ZuoDHHp-rW",
        "colab_type": "code",
        "outputId": "3424bb2e-1d28-48e7-b211-e6a7f6aa5a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "s = 'hello\\r\\nhow are you doing\\r\\nI\\tam\\tdoing\\tgreat\\r\\n:)'\n",
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\r\n",
            "how are you doing\r\n",
            "I\tam\tdoing\tgreat\r\n",
            ":)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q-39kiGp-rX",
        "colab_type": "code",
        "outputId": "166f9bd0-cce3-490c-9da4-40dd0c7036ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s.translate(s.maketrans(\"\\n\\t\\r\", \"   \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello  how are you doing  I am doing great  :)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZ3R1atp-rZ",
        "colab_type": "text"
      },
      "source": [
        "## Your Turn: Add in all the necessary functions and build your pre-processor!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFzUb7ZHp-rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
        "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
        "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
        "                       stopword_list=None):\n",
        "    \n",
        "    # strip HTML\n",
        "    if html_strip:\n",
        "        _____\n",
        "    \n",
        "    # remove extra newlines (often might be present in really noisy text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    \n",
        "    # remove accented characters\n",
        "    _____\n",
        "    \n",
        "    # expand contractions    \n",
        "    _____\n",
        "        \n",
        "    \n",
        "    # lemmatize text\n",
        "    _____\n",
        "        \n",
        "    # remove special characters and\\or digits    \n",
        "    if special_char_removal:\n",
        "        # insert spaces between special characters to isolate them    \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
        "        _____  \n",
        "        \n",
        "    # stem text\n",
        "    if text_stemming and not text_lemmatization:\n",
        "        _____\n",
        "        \n",
        "    # lowercase the text    \n",
        "    if text_lower_case:\n",
        "        _____\n",
        "        \n",
        "        \n",
        "    # remove stopwords\n",
        "    if stopword_removal:\n",
        "        _____\n",
        "        \n",
        "    # remove extra whitespace\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn1lkFJQp-rb",
        "colab_type": "text"
      },
      "source": [
        "# Test on a single document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdyRHLjQp-rb",
        "colab_type": "code",
        "outputId": "c6708782-e9f1-441b-ae10-e847db04a4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\"\n",
        "print(document)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\r\n",
            " \n",
            "              It's an amazing language which can be used for [Scripting\tWeb development\tBackend development],\r\n",
            "\r\n",
            "\n",
            "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\n",
            "\n",
            "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\n",
            "\n",
            "\n",
            "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
            "           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsBdFt10p-rd",
        "colab_type": "code",
        "outputId": "db5d85da-29b3-4f0e-a4cf-48a97d7438cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "text_pre_processor(document)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2b5270ffeb41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_pre_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e5586dc83249>\u001b[0m in \u001b[0;36mtext_pre_processor\u001b[0;34m(text, html_strip, accented_char_removal, contraction_expansion, text_lower_case, text_stemming, text_lemmatization, special_char_removal, remove_digits, stopword_removal, stopword_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# strip HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhtml_strip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0m_____\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# remove extra newlines (often might be present in really noisy text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fO40shBp-re",
        "colab_type": "text"
      },
      "source": [
        "# Test on a corpus of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzfL67q4p-rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "corpus_pre_processor = np.vectorize(text_pre_processor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqIFi6w8p-rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\",\n",
        "          \"\"\"US unveils world's most powerful supercomputer, beats China. \n",
        "             The US has unveiled the world's most powerful supercomputer \n",
        "             called 'Summit', beating the previous record-holder China's Sunway \n",
        "             TaihuLight. With a peak performance of 200,000 trillion calculations \n",
        "             per second, it is over twice as fast as Sunway TaihuLight, which is capable \n",
        "             of 93,000 trillion calculations per second. Summit has 4,608 servers, \n",
        "             which reportedly take up the size of two tennis courts.\"\"\",\n",
        "          \"\"\"The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. \n",
        "            The story began as a sequel to Tolkien's 1937 fantasy novel The Hobbit, but eventually developed into \n",
        "            a much larger work. Written in stages between 1937 and 1949, The Lord of the Rings is one of the \n",
        "            best-selling novels ever written, with over 150 million copies sold.[1]\n",
        "          \"\"\",\n",
        "          \"\"\"The title of the novel refers to the story's main antagonist, the Dark Lord Sauron,[a] \n",
        "             who had in an earlier age created the One Ring to rule the other Rings of Power as the ultimate weapon \n",
        "             in his campaign to conquer and rule all of Middle-earth. From quiet beginnings in the Shire, a hobbit \n",
        "             land not unlike the English countryside, the story ranges across Middle-earth, following the course \n",
        "             of the War of the Ring through the eyes of its characters, not only the hobbits Frodo Baggins, \n",
        "             Samwise \"Sam\" Gamgee, Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took, but also the hobbits' \n",
        "             chief allies and travelling companions: the Men, Aragorn, a Ranger of the North, and Boromir, \n",
        "             a Captain of Gondor; Gimli, a Dwarf warrior; Legolas Greenleaf, an Elven prince; and Gandalf, a wizard.\n",
        "          \"\"\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnd-MV0Yp-ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for doc in corpus_pre_processor(corpus):\n",
        "    print(doc)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}